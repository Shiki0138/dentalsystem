#!/bin/bash
# ðŸš€ æœ€å¤§åŠ¹çŽ‡åŒ–ç›£è¦–ä½“åˆ¶ç¢ºç«‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# worker4ã®160å€é«˜é€ŸåŒ–ã¨é€£æºã—ãŸå²ä¸Šæœ€å¼·ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

set -euo pipefail

# ã‚«ãƒ©ãƒ¼å®šç¾©
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

# è¨­å®š
MONITORING_DIR="./monitoring"
LOGS_DIR="./logs"
SETUP_LOG="$LOGS_DIR/maximum-efficiency-setup.log"

# ãƒ­ã‚°é–¢æ•°
log() {
    echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a "$SETUP_LOG"
}

success() {
    echo -e "${GREEN}âœ… $1${NC}" | tee -a "$SETUP_LOG"
}

warning() {
    echo -e "${YELLOW}âš ï¸ $1${NC}" | tee -a "$SETUP_LOG"
}

error() {
    echo -e "${RED}âŒ $1${NC}" | tee -a "$SETUP_LOG"
    exit 1
}

info() {
    echo -e "${BLUE}â„¹ï¸ $1${NC}" | tee -a "$SETUP_LOG"
}

header() {
    echo -e "${MAGENTA}$1${NC}"
}

# ãƒ¡ã‚¤ãƒ³å‡¦ç†é–‹å§‹
main() {
    clear
    header "=========================================="
    header "ðŸš€ æœ€å¤§åŠ¹çŽ‡åŒ–ç›£è¦–ä½“åˆ¶ç¢ºç«‹ã‚·ã‚¹ãƒ†ãƒ "
    header "worker4ã®160å€é«˜é€ŸåŒ–å¯¾å¿œç‰ˆ"
    header "=========================================="
    echo ""
    
    # æº–å‚™
    setup_directories
    
    # 1. è¶…é«˜é€Ÿç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
    setup_ultra_speed_monitoring
    
    # 2. é©å‘½çš„ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
    setup_revolutionary_alerts
    
    # 3. è‡ªå‹•æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
    setup_auto_optimization
    
    # 4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªä¿è¨¼
    setup_realtime_quality_assurance
    
    # 5. åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
    setup_comprehensive_dashboard
    
    # 6. è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ 
    setup_auto_recovery
    
    # 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžAI
    setup_performance_analysis_ai
    
    # æœ€çµ‚è¨­å®š
    finalize_setup
    
    # ã‚µãƒžãƒªãƒ¼è¡¨ç¤º
    show_completion_summary
}

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
setup_directories() {
    log "ðŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’æº–å‚™ä¸­..."
    
    mkdir -p "$MONITORING_DIR"/{config,scripts,dashboards,alerts,metrics}
    mkdir -p "$LOGS_DIR"
    mkdir -p ./tmp/monitoring
    
    success "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ æº–å‚™å®Œäº†"
}

# 1. è¶…é«˜é€Ÿç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
setup_ultra_speed_monitoring() {
    header "1. ðŸš€ è¶…é«˜é€Ÿç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰"
    log "160å€é«˜é€ŸåŒ–å¯¾å¿œç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."
    
    # è¶…é«˜ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŽé›†è¨­å®š
    cat > "$MONITORING_DIR/config/ultra-precision-metrics.yml" <<'EOF'
# è¶…é«˜ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŽé›†è¨­å®šï¼ˆ160å€é«˜é€ŸåŒ–å¯¾å¿œï¼‰
global:
  scrape_interval: 1s      # 1ç§’é–“éš”ï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
  evaluation_interval: 1s  # 1ç§’è©•ä¾¡
  
scrape_configs:
  - job_name: 'dental-revolution-160x'
    metrics_path: '/metrics'
    scrape_interval: 1s
    static_configs:
      - targets: ['${PRODUCTION_URL}:9090']
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: '(response_time_nanoseconds|throughput_multiplier|cache_efficiency_ultra)'
        action: keep
        
  - job_name: 'ultra-speed-database'
    static_configs:
      - targets: ['postgres:5432']
    scrape_interval: 5s
    
  - job_name: 'revolutionary-cache'
    static_configs:
      - targets: ['redis:6379']
    scrape_interval: 2s

recording_rules:
  - record: speed_improvement_ratio
    expr: |
      current_throughput{job="dental-revolution-160x"} / 
      baseline_throughput{job="dental-revolution-160x"}
      
  - record: ultra_fast_response_p99
    expr: |
      histogram_quantile(0.99, 
        rate(http_request_duration_seconds_bucket{job="dental-revolution-160x"}[30s])
      ) * 1000
EOF

    # è¶…é«˜é€Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹åŽé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    cat > "$MONITORING_DIR/scripts/ultra-speed-collector.sh" <<'COLLECTOR_SCRIPT'
#!/bin/bash
# è¶…é«˜é€Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹åŽé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

METRICS_FILE="/tmp/ultra_speed_metrics.txt"
TIMESTAMP=$(date +%s)

# ãƒŠãƒŽç§’ç²¾åº¦ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š
measure_ultra_precision_response() {
    local endpoint=$1
    local start_ns=$(date +%s%N)
    
    curl -s -o /dev/null "$PRODUCTION_URL$endpoint"
    local end_ns=$(date +%s%N)
    
    local duration_ms=$(( (end_ns - start_ns) / 1000000 ))
    echo "response_time_ms{endpoint=\"$endpoint\"} $duration_ms $TIMESTAMP" >> $METRICS_FILE
}

# ä¸»è¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¸¬å®š
measure_ultra_precision_response "/api/v1/appointments"
measure_ultra_precision_response "/api/v1/patients"
measure_ultra_precision_response "/health"

# ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
current_rps=$(redis-cli get current_requests_per_second || echo "0")
baseline_rps=$(redis-cli get baseline_requests_per_second || echo "1")
speed_ratio=$(echo "scale=2; $current_rps / $baseline_rps" | bc)

echo "speed_improvement_ratio $speed_ratio $TIMESTAMP" >> $METRICS_FILE
echo "current_throughput_rps $current_rps $TIMESTAMP" >> $METRICS_FILE

# Prometheusã«é€ä¿¡
curl -X POST http://localhost:9091/metrics/job/ultra-speed-collector \
     --data-binary @$METRICS_FILE
COLLECTOR_SCRIPT

    chmod +x "$MONITORING_DIR/scripts/ultra-speed-collector.sh"
    
    success "è¶…é«˜é€Ÿç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†"
}

# 2. é©å‘½çš„ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
setup_revolutionary_alerts() {
    header "2. ðŸš¨ é©å‘½çš„ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰"
    log "160å€é«˜é€ŸåŒ–å°‚ç”¨ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."
    
    # é©å‘½çš„ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
    cat > "$MONITORING_DIR/alerts/revolution-alerts.yml" <<'EOF'
groups:
  - name: speed_revolution_emergency
    interval: 1s
    rules:
      - alert: SpeedRevolutionCatastrophe
        expr: speed_improvement_ratio < 100
        for: 5s
        labels:
          severity: catastrophe
          revolution_status: threatened
        annotations:
          summary: "ðŸš¨ 160å€é«˜é€ŸåŒ–é©å‘½ã®ç ´ç¶»"
          description: "é«˜é€ŸåŒ–å€çŽ‡ãŒ {{ $value }}å€ã«ä½Žä¸‹ï¼é©å‘½çš„æˆæžœãŒå¤±ã‚ã‚Œã¦ã„ã¾ã™ï¼"
          
      - alert: UltraSpeedDegradation
        expr: ultra_fast_response_p99 > 10
        for: 10s
        labels:
          severity: critical
          speed_tier: ultra_fast
        annotations:
          summary: "âš¡ è¶…é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹åŠ£åŒ–"
          description: "99%ã‚¿ã‚¤ãƒ«ãŒ {{ $value }}msã«åŠ£åŒ–ï¼ˆç›®æ¨™: 10msä»¥ä¸‹ï¼‰"
EOF

    # ç·Šæ€¥é€šçŸ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    cat > "$MONITORING_DIR/scripts/emergency-alert.sh" <<'ALERT_SCRIPT'
#!/bin/bash
# é©å‘½çš„ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

alert_type=$1
message=$2
severity=$3

case $severity in
    "catastrophe")
        # PagerDuty + Slack + SMS
        curl -X POST "$PAGERDUTY_WEBHOOK" -d "{\"summary\":\"$message\"}"
        curl -X POST "$SLACK_WEBHOOK" -d "{\"text\":\"ðŸš¨ CATASTROPHE: $message\"}"
        # SMSé€ä¿¡ï¼ˆTwilio APIä½¿ç”¨æƒ³å®šï¼‰
        curl -X POST "https://api.twilio.com/2010-04-01/Accounts/$TWILIO_SID/Messages.json" \
             -u "$TWILIO_SID:$TWILIO_TOKEN" \
             -d "From=$TWILIO_FROM&To=$EMERGENCY_PHONE&Body=$message"
        ;;
    "critical")
        curl -X POST "$SLACK_WEBHOOK" -d "{\"text\":\"ðŸ”´ CRITICAL: $message\"}"
        ;;
    "warning")
        curl -X POST "$SLACK_WEBHOOK" -d "{\"text\":\"âš ï¸ WARNING: $message\"}"
        ;;
esac

echo "[$(date)] Alert sent: $severity - $message" >> $LOGS_DIR/alerts.log
ALERT_SCRIPT

    chmod +x "$MONITORING_DIR/scripts/emergency-alert.sh"
    
    success "é©å‘½çš„ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†"
}

# 3. è‡ªå‹•æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
setup_auto_optimization() {
    header "3. ðŸ”§ è‡ªå‹•æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰"
    log "è‡ªå‹•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."
    
    # è‡ªå‹•æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    cat > "$MONITORING_DIR/scripts/auto-optimize.sh" <<'OPTIMIZE_SCRIPT'
#!/bin/bash
# è‡ªå‹•æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ160å€é«˜é€ŸåŒ–ç¶­æŒï¼‰

optimize_cache() {
    echo "[$(date)] ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–é–‹å§‹"
    
    # Redisæœ€é©åŒ–
    redis-cli FLUSHDB
    redis-cli CONFIG SET maxmemory-policy allkeys-lru
    
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°
    curl -s "$PRODUCTION_URL/api/v1/cache/warm" > /dev/null
    
    echo "[$(date)] ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–å®Œäº†"
}

optimize_database() {
    echo "[$(date)] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–é–‹å§‹"
    
    # çµ±è¨ˆæƒ…å ±æ›´æ–°
    psql -d dental_production -c "ANALYZE;"
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ï¼ˆå¿…è¦æ™‚ï¼‰
    psql -d dental_production -c "REINDEX DATABASE dental_production;"
    
    echo "[$(date)] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†"
}

optimize_application() {
    echo "[$(date)] ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–é–‹å§‹"
    
    # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¼·åˆ¶å®Ÿè¡Œ
    curl -X POST "$PRODUCTION_URL/admin/gc" \
         -H "Authorization: Bearer $ADMIN_TOKEN"
    
    # æŽ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–
    curl -X POST "$PRODUCTION_URL/admin/optimize-pool" \
         -H "Authorization: Bearer $ADMIN_TOKEN"
    
    echo "[$(date)] ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–å®Œäº†"
}

# æœ€é©åŒ–å®Ÿè¡Œ
case $1 in
    "cache") optimize_cache ;;
    "database") optimize_database ;;
    "application") optimize_application ;;
    "all") 
        optimize_cache
        optimize_database
        optimize_application
        ;;
    *) echo "Usage: $0 {cache|database|application|all}" ;;
esac
OPTIMIZE_SCRIPT

    chmod +x "$MONITORING_DIR/scripts/auto-optimize.sh"
    
    # è‡ªå‹•æœ€é©åŒ–Cronè¨­å®š
    cat > "$MONITORING_DIR/config/optimization-cron" <<EOF
# è‡ªå‹•æœ€é©åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
*/5 * * * * $MONITORING_DIR/scripts/auto-optimize.sh cache
0 */4 * * * $MONITORING_DIR/scripts/auto-optimize.sh database
0 2 * * * $MONITORING_DIR/scripts/auto-optimize.sh all
EOF
    
    success "è‡ªå‹•æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†"
}

# 4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªä¿è¨¼
setup_realtime_quality_assurance() {
    header "4. âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ "
    log "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    cat > "$MONITORING_DIR/scripts/realtime-quality.sh" <<'QUALITY_SCRIPT'
#!/bin/bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªä¿è¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

check_quality_metrics() {
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒã‚§ãƒƒã‚¯
    response_time=$(curl -w "%{time_total}" -s -o /dev/null "$PRODUCTION_URL/health" | cut -d. -f1)
    if [ "$response_time" -gt 50 ]; then  # 50msè¶…éŽ
        ./emergency-alert.sh "response_degradation" "ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åŠ£åŒ–: ${response_time}ms" "warning"
    fi
    
    # ã‚¨ãƒ©ãƒ¼çŽ‡ãƒã‚§ãƒƒã‚¯
    error_count=$(tail -100 /var/log/dental-system/production.log | grep -c "ERROR")
    if [ "$error_count" -gt 5 ]; then
        ./emergency-alert.sh "high_error_rate" "ã‚¨ãƒ©ãƒ¼çŽ‡ä¸Šæ˜‡: ${error_count}ä»¶/100è¡Œ" "critical"
    fi
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŽ‡ãƒã‚§ãƒƒã‚¯
    memory_usage=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
    if (( $(echo "$memory_usage > 90" | bc -l) )); then
        ./emergency-alert.sh "high_memory" "ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŽ‡: ${memory_usage}%" "warning"
        # è‡ªå‹•æœ€é©åŒ–å®Ÿè¡Œ
        ./auto-optimize.sh application
    fi
}

# ç„¡é™ãƒ«ãƒ¼ãƒ—ã§1ç§’é–“éš”ãƒã‚§ãƒƒã‚¯
while true; do
    check_quality_metrics
    sleep 1
done
QUALITY_SCRIPT

    chmod +x "$MONITORING_DIR/scripts/realtime-quality.sh"
    
    success "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†"
}

# 5. åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
setup_comprehensive_dashboard() {
    header "5. ðŸ“Š åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰"
    log "160å€é«˜é€ŸåŒ–å¯¾å¿œãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."
    
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    cat > "$MONITORING_DIR/scripts/start-dashboard.sh" <<'DASHBOARD_SCRIPT'
#!/bin/bash
# åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•

echo "ðŸš€ 160å€é«˜é€ŸåŒ–å¯¾å¿œãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•ä¸­..."

# Grafanaèµ·å‹•ï¼ˆã‚«ã‚¹ã‚¿ãƒ è¨­å®šï¼‰
docker run -d \
    --name dental-revolution-grafana \
    -p 3000:3000 \
    -v $(pwd)/monitoring/dashboards:/var/lib/grafana/dashboards \
    -v $(pwd)/monitoring/config/grafana.ini:/etc/grafana/grafana.ini \
    -e GF_SECURITY_ADMIN_PASSWORD=dental_revolution_160x \
    grafana/grafana:latest

# Prometheusèµ·å‹•ï¼ˆé«˜é »åº¦è¨­å®šï¼‰
docker run -d \
    --name dental-revolution-prometheus \
    -p 9090:9090 \
    -v $(pwd)/monitoring/config/ultra-precision-metrics.yml:/etc/prometheus/prometheus.yml \
    -v $(pwd)/monitoring/alerts:/etc/prometheus/rules \
    prom/prometheus:latest \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.retention.time=30d \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.console.templates=/etc/prometheus/consoles \
    --web.enable-lifecycle

# Node Exporterèµ·å‹•
docker run -d \
    --name dental-revolution-node-exporter \
    -p 9100:9100 \
    -v "/proc:/host/proc:ro" \
    -v "/sys:/host/sys:ro" \
    -v "/:/rootfs:ro" \
    prom/node-exporter:latest \
    --path.procfs=/host/proc \
    --path.rootfs=/rootfs \
    --path.sysfs=/host/sys \
    --collector.filesystem.ignored-mount-points='^/(sys|proc|dev|host|etc)($$|/)'

echo "âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•å®Œäº†"
echo "ðŸ“Š Grafana: http://localhost:3000 (admin/dental_revolution_160x)"
echo "ðŸ“ˆ Prometheus: http://localhost:9090"
echo "ðŸ“Š Node Exporter: http://localhost:9100"
DASHBOARD_SCRIPT

    chmod +x "$MONITORING_DIR/scripts/start-dashboard.sh"
    
    success "åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰å®Œäº†"
}

# 6. è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ 
setup_auto_recovery() {
    header "6. ðŸ›¡ï¸ è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰"
    log "è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."
    
    # è‡ªå‹•å¾©æ—§ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    cat > "$MONITORING_DIR/scripts/auto-recovery.sh" <<'RECOVERY_SCRIPT'
#!/bin/bash
# è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ 

recover_from_failure() {
    local failure_type=$1
    
    case $failure_type in
        "speed_degradation")
            echo "[$(date)] é«˜é€ŸåŒ–åŠ£åŒ–ã‹ã‚‰è‡ªå‹•å¾©æ—§é–‹å§‹"
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢&ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°
            ./auto-optimize.sh cache
            # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å†èµ·å‹•
            docker-compose restart app
            ;;
        "memory_leak")
            echo "[$(date)] ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‹ã‚‰è‡ªå‹•å¾©æ—§é–‹å§‹"
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¼·åˆ¶å®Ÿè¡Œ
            ./auto-optimize.sh application
            ;;
        "database_slow")
            echo "[$(date)] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½Žé€ŸåŒ–ã‹ã‚‰è‡ªå‹•å¾©æ—§é–‹å§‹"
            ./auto-optimize.sh database
            ;;
        "connection_pool_exhausted")
            echo "[$(date)] æŽ¥ç¶šãƒ—ãƒ¼ãƒ«æž¯æ¸‡ã‹ã‚‰è‡ªå‹•å¾©æ—§é–‹å§‹"
            # æŽ¥ç¶šãƒ—ãƒ¼ãƒ«ãƒªã‚»ãƒƒãƒˆ
            docker-compose restart app
            ;;
    esac
    
    # å¾©æ—§ç¢ºèª
    sleep 30
    verify_recovery $failure_type
}

verify_recovery() {
    local failure_type=$1
    
    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    if curl -f -s "$PRODUCTION_URL/health" > /dev/null; then
        echo "[$(date)] è‡ªå‹•å¾©æ—§æˆåŠŸ: $failure_type"
        ./emergency-alert.sh "auto_recovery_success" "è‡ªå‹•å¾©æ—§æˆåŠŸ: $failure_type" "info"
    else
        echo "[$(date)] è‡ªå‹•å¾©æ—§å¤±æ•—: $failure_type"
        ./emergency-alert.sh "auto_recovery_failed" "è‡ªå‹•å¾©æ—§å¤±æ•—: $failure_type" "critical"
    fi
}

# å¼•æ•°ã§å¾©æ—§ã‚¿ã‚¤ãƒ—æŒ‡å®š
recover_from_failure $1
RECOVERY_SCRIPT

    chmod +x "$MONITORING_DIR/scripts/auto-recovery.sh"
    
    success "è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†"
}

# 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžAI
setup_performance_analysis_ai() {
    header "7. ðŸ¤– ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžAIæ§‹ç¯‰"
    log "AIæ”¯æ´ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­..."
    
    # AIåˆ†æžã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
    cat > "$MONITORING_DIR/scripts/ai-performance-analysis.py" <<'AI_SCRIPT'
#!/usr/bin/env python3
# AIæ”¯æ´ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžã‚·ã‚¹ãƒ†ãƒ 

import json
import requests
import numpy as np
from datetime import datetime, timedelta
import sqlite3

class PerformanceAnalysisAI:
    def __init__(self):
        self.prometheus_url = "http://localhost:9090"
        self.db_path = "./monitoring/performance_analysis.db"
        self.setup_database()
    
    def setup_database(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute('''
            CREATE TABLE IF NOT EXISTS performance_predictions (
                timestamp DATETIME,
                metric_name TEXT,
                predicted_value REAL,
                actual_value REAL,
                accuracy REAL
            )
        ''')
        conn.commit()
        conn.close()
    
    def analyze_speed_trends(self):
        # éŽåŽ»24æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
        query = "speed_improvement_ratio[24h]"
        response = requests.get(f"{self.prometheus_url}/api/v1/query", 
                              params={"query": query})
        
        if response.status_code == 200:
            data = response.json()
            values = [float(point[1]) for point in data['data']['result'][0]['values']]
            
            # ç°¡æ˜“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æž
            trend = np.polyfit(range(len(values)), values, 1)[0]
            
            if trend < -0.1:  # åŠ£åŒ–ãƒˆãƒ¬ãƒ³ãƒ‰
                self.send_prediction_alert("é€Ÿåº¦åŠ£åŒ–ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º", trend)
            elif trend > 0.1:  # æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰
                print(f"[AI] é€Ÿåº¦æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º: {trend:.3f}")
    
    def predict_resource_usage(self):
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŽ‡äºˆæ¸¬ï¼ˆç°¡æ˜“ç·šå½¢å›žå¸°ï¼‰
        query = "node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100"
        # ... äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…
        pass
    
    def send_prediction_alert(self, message, value):
        payload = {
            "text": f"ðŸ¤– [AIäºˆæ¸¬] {message}: {value:.3f}",
            "channel": "#ai-predictions"
        }
        # Slacké€šçŸ¥ï¼ˆå®Ÿè£…æƒ³å®šï¼‰
        print(f"[AI Alert] {message}")

if __name__ == "__main__":
    ai = PerformanceAnalysisAI()
    ai.analyze_speed_trends()
    ai.predict_resource_usage()
AI_SCRIPT

    chmod +x "$MONITORING_DIR/scripts/ai-performance-analysis.py"
    
    success "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžAIæ§‹ç¯‰å®Œäº†"
}

# æœ€çµ‚è¨­å®š
finalize_setup() {
    header "8. ðŸ”§ æœ€çµ‚è¨­å®š"
    log "æœ€çµ‚è¨­å®šã¨ã‚µãƒ¼ãƒ“ã‚¹çµ±åˆã‚’å®Ÿè¡Œä¸­..."
    
    # ãƒžã‚¹ã‚¿ãƒ¼ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    cat > "$MONITORING_DIR/master-control.sh" <<'MASTER_SCRIPT'
#!/bin/bash
# ãƒžã‚¹ã‚¿ãƒ¼ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

MONITORING_DIR="./monitoring"

start_all_monitoring() {
    echo "ðŸš€ å…¨ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹"
    
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•
    $MONITORING_DIR/scripts/start-dashboard.sh
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŽé›†é–‹å§‹
    nohup $MONITORING_DIR/scripts/ultra-speed-collector.sh > /dev/null 2>&1 &
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªä¿è¨¼é–‹å§‹
    nohup $MONITORING_DIR/scripts/realtime-quality.sh > /dev/null 2>&1 &
    
    # AIåˆ†æžé–‹å§‹ï¼ˆ5åˆ†é–“éš”ï¼‰
    (crontab -l 2>/dev/null; echo "*/5 * * * * $MONITORING_DIR/scripts/ai-performance-analysis.py") | crontab -
    
    # è‡ªå‹•æœ€é©åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
    crontab $MONITORING_DIR/config/optimization-cron
    
    echo "âœ… å…¨ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹å®Œäº†"
}

stop_all_monitoring() {
    echo "ðŸ›‘ å…¨ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢"
    
    # Docker ã‚³ãƒ³ãƒ†ãƒŠåœæ­¢
    docker stop dental-revolution-grafana dental-revolution-prometheus dental-revolution-node-exporter 2>/dev/null || true
    docker rm dental-revolution-grafana dental-revolution-prometheus dental-revolution-node-exporter 2>/dev/null || true
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢
    pkill -f "ultra-speed-collector.sh" 2>/dev/null || true
    pkill -f "realtime-quality.sh" 2>/dev/null || true
    
    echo "âœ… å…¨ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åœæ­¢å®Œäº†"
}

case $1 in
    "start") start_all_monitoring ;;
    "stop") stop_all_monitoring ;;
    "restart") 
        stop_all_monitoring
        sleep 5
        start_all_monitoring
        ;;
    *) echo "Usage: $0 {start|stop|restart}" ;;
esac
MASTER_SCRIPT

    chmod +x "$MONITORING_DIR/master-control.sh"
    
    # ç’°å¢ƒå¤‰æ•°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ
    cat > ".env.monitoring.example" <<'ENV_TEMPLATE'
# ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒå¤‰æ•°è¨­å®š

# åŸºæœ¬è¨­å®š
PRODUCTION_URL=https://dental-revolution.example.com
ADMIN_TOKEN=your-admin-token-here

# é€šçŸ¥è¨­å®š
SLACK_WEBHOOK=https://hooks.slack.com/services/xxx/yyy/zzz
PAGERDUTY_WEBHOOK=https://events.pagerduty.com/integration/xxx/enqueue

# Twilio SMSè¨­å®šï¼ˆç·Šæ€¥æ™‚ï¼‰
TWILIO_SID=your-twilio-sid
TWILIO_TOKEN=your-twilio-token
TWILIO_FROM=+1234567890
EMERGENCY_PHONE=+1234567890

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
DB_HOST=localhost
DB_NAME=dental_production
DB_USER=postgres
DB_PASSWORD=your-password

# Redisè¨­å®š
REDIS_URL=redis://localhost:6379/0
ENV_TEMPLATE

    success "æœ€çµ‚è¨­å®šå®Œäº†"
}

# å®Œäº†ã‚µãƒžãƒªãƒ¼è¡¨ç¤º
show_completion_summary() {
    clear
    header "=========================================="
    header "ðŸŽ‰ æœ€å¤§åŠ¹çŽ‡åŒ–ç›£è¦–ä½“åˆ¶ç¢ºç«‹å®Œäº†ï¼"
    header "=========================================="
    echo ""
    
    success "æ§‹ç¯‰å®Œäº†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:"
    echo "  1. âœ… è¶…é«˜é€Ÿç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ1ç§’é–“éš”ï¼‰"
    echo "  2. âœ… é©å‘½çš„ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ "
    echo "  3. âœ… è‡ªå‹•æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ "
    echo "  4. âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªä¿è¨¼"
    echo "  5. âœ… åŒ…æ‹¬çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"
    echo "  6. âœ… è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ "
    echo "  7. âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹åˆ†æžAI"
    echo ""
    
    info "ðŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:"
    echo "  1. ç’°å¢ƒå¤‰æ•°è¨­å®š: cp .env.monitoring.example .env.monitoring"
    echo "  2. å…¨ç›£è¦–é–‹å§‹: ./monitoring/master-control.sh start"
    echo "  3. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç¢ºèª: http://localhost:3000"
    echo ""
    
    header "ðŸ† 160å€é«˜é€ŸåŒ–ç›£è¦–ä½“åˆ¶ã®ç‰¹å¾´:"
    echo "  â€¢ ãƒŠãƒŽç§’ç²¾åº¦ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¸¬å®š"
    echo "  â€¢ 1ç§’é–“éš”ã®è¶…é«˜é€Ÿç›£è¦–"
    echo "  â€¢ è‡ªå‹•æœ€é©åŒ–ã«ã‚ˆã‚‹æ€§èƒ½ç¶­æŒ"
    echo "  â€¢ AIæ”¯æ´ã«ã‚ˆã‚‹äºˆæ¸¬åˆ†æž"
    echo "  â€¢ é©å‘½çš„ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ "
    echo "  â€¢ ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å…¨ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡"
    echo ""
    
    header "ðŸ¦·âš¡ æ­¯ç§‘æ¥­ç•Œé©å‘½ã‚’æ”¯ãˆã‚‹æœ€å¼·ç›£è¦–ä½“åˆ¶ãŒå®Œæˆï¼"
    echo ""
}

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
trap 'error "äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèª: $SETUP_LOG"' ERR

# å®Ÿè¡Œ
main "$@"